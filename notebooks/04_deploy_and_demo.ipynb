{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4c3f397d-3803-4dad-a640-c0c3bc584d3e",
   "metadata": {},
   "source": [
    "# Phase 4 — Deployment & Live Demo (Flask API + Streamlit UI)\n",
    "\n",
    "Goal:\n",
    "- Export batch predictions for Tableau.\n",
    "- Provide a REST API for live prediction.\n",
    "- Provide a Streamlit demo UI (single + batch prediction).\n",
    "- Provide deployment instructions (Render for Flask, Streamlit Cloud for UI).\n",
    "\n",
    "Assumptions:\n",
    "- You have a trained model file inside `models/` (we will try to detect typical names).\n",
    "- You have cleaned dataset at `data/cleaned/telco_churn_clean.csv`.\n",
    "- `models/feature_columns.json` or similar may exist; if not the notebook will derive it.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9aa8fc07-62e8-439f-bd22-f6575d088d5b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Repo root: /Users/hrithik/customer-churn-Analysis\n",
      "Cleaned data exists: True\n",
      "Models dir: True\n",
      "API dir: True\n"
     ]
    }
   ],
   "source": [
    "# Imports & Path setup\n",
    "from pathlib import Path\n",
    "import json, joblib, os, sys\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "REPO_ROOT = Path(\".\").resolve()\n",
    "MODEL_DIR = REPO_ROOT / \"models\"\n",
    "DATA_CLEANED = REPO_ROOT / \"data\" / \"cleaned\" / \"Telco-Customer-Churn.csv\"\n",
    "ANALYSIS_DIR = REPO_ROOT / \"data\" / \"analysis\"\n",
    "API_DIR = REPO_ROOT / \"api\"\n",
    "\n",
    "ANALYSIS_DIR.mkdir(parents=True, exist_ok=True)\n",
    "API_DIR.mkdir(parents=True, exist_ok=True)\n",
    "MODEL_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "print(\"Repo root:\", REPO_ROOT)\n",
    "print(\"Cleaned data exists:\", DATA_CLEANED.exists())\n",
    "print(\"Models dir:\", MODEL_DIR.exists())\n",
    "print(\"API dir:\", API_DIR.exists())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acbc115b-2439-4a12-8684-52084bf38ff9",
   "metadata": {},
   "source": [
    "## 1) Locate & load the trained model\n",
    "We try common filenames (`churn_model.pkl`, `churn_model_rf.pkl`, `churn_model_xgb.pkl`, etc.).  \n",
    "If missing, stop and run Phase 3 to re-save the model.\n",
    "We also prepare `feature_columns.json` (list of final dummy column names). If missing, derive it from the cleaned data using the same one-hot approach.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b8d89e28-bfb3-4645-94f9-835ae450ad48",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using model: /Users/hrithik/customer-churn-Analysis/models/churn_model.pkl\n",
      "Model loaded. Type: <class 'xgboost.sklearn.XGBClassifier'>\n",
      "Loaded 13601 feature columns from /Users/hrithik/customer-churn-Analysis/models/feature_columns.json\n"
     ]
    }
   ],
   "source": [
    "# Cell 2 — Find model file (robust)\n",
    "possible_models = [\n",
    "    MODEL_DIR / \"churn_model.pkl\",\n",
    "    MODEL_DIR / \"churn_model_rf.pkl\",\n",
    "    MODEL_DIR / \"churn_model_xgb.pkl\",\n",
    "    MODEL_DIR / \"churn_model_rf_v1.pkl\",\n",
    "    MODEL_DIR / \"churn_model_rf_v2.pkl\",\n",
    "    MODEL_DIR / \"model.pkl\"\n",
    "]\n",
    "model_path = None\n",
    "for p in possible_models:\n",
    "    if p.exists():\n",
    "        model_path = p\n",
    "        break\n",
    "\n",
    "# if none found look for any .pkl under models/\n",
    "if model_path is None:\n",
    "    pkls = list(MODEL_DIR.glob(\"*.pkl\"))\n",
    "    if pkls:\n",
    "        model_path = pkls[0]\n",
    "\n",
    "if model_path is None:\n",
    "    raise FileNotFoundError(\"No model .pkl found in models/. Please run Phase 3 and save the model to models/\")\n",
    "\n",
    "print(\"Using model:\", model_path)\n",
    "\n",
    "# load model\n",
    "model = joblib.load(model_path)\n",
    "print(\"Model loaded. Type:\", type(model))\n",
    "\n",
    "# feature columns file\n",
    "FEATURE_PATH = MODEL_DIR / \"feature_columns.json\"\n",
    "if not FEATURE_PATH.exists():\n",
    "    if not DATA_CLEANED.exists():\n",
    "        raise FileNotFoundError(\"Cleaned dataset not found at data/cleaned/telco_churn_clean.csv. Run Phase 2 first.\")\n",
    "    dfc = pd.read_csv(DATA_CLEANED)\n",
    "    X_sample = dfc.drop(columns=[\"churn\"], errors=\"ignore\")\n",
    "    X_dummies = pd.get_dummies(X_sample, drop_first=True)\n",
    "    FEATURE_COLS = X_dummies.columns.tolist()\n",
    "    FEATURE_PATH.write_text(json.dumps(FEATURE_COLS), encoding=\"utf-8\")\n",
    "    print(f\"Derived and saved {len(FEATURE_COLS)} feature columns to {FEATURE_PATH}\")\n",
    "else:\n",
    "    FEATURE_COLS = json.loads(FEATURE_PATH.read_text(encoding=\"utf-8\"))\n",
    "    print(f\"Loaded {len(FEATURE_COLS)} feature columns from {FEATURE_PATH}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cba994d2-c8eb-4d38-bb3f-c2c5d61640b8",
   "metadata": {},
   "source": [
    "## Helper: prepare_input(df)\n",
    "Make a robust function that:\n",
    "- strips strings,\n",
    "- one-hot encodes (drop_first=True) like training,\n",
    "- reindexes to the saved FEATURE_COLS (fills missing columns with 0).\n",
    "This removes performance warnings and ensures exact feature alignment.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "12e7a52e-13c1-45d2-bcba-e7744025185e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prepared shape: (2, 13601)\n"
     ]
    }
   ],
   "source": [
    "# prepare_input implementation (optimized)\n",
    "def prepare_input(df_input: pd.DataFrame, feature_cols=FEATURE_COLS) -> pd.DataFrame:\n",
    "    df = df_input.copy()\n",
    "    # basic cleaning\n",
    "    for c in df.select_dtypes(include=\"object\").columns:\n",
    "        df[c] = df[c].astype(str).str.strip()\n",
    "    X = pd.get_dummies(df, drop_first=True)\n",
    "    # reindex to target columns (this fills missing cols with 0 and drops extras)\n",
    "    X = X.reindex(columns=feature_cols, fill_value=0)\n",
    "    return X\n",
    "\n",
    "# quick sanity check on a small sample\n",
    "if DATA_CLEANED.exists():\n",
    "    test_row = pd.read_csv(DATA_CLEANED).drop(columns=[\"churn\"], errors=\"ignore\").head(2)\n",
    "    Xt = prepare_input(test_row)\n",
    "    print(\"Prepared shape:\", Xt.shape)\n",
    "else:\n",
    "    print(\"No cleaned data to test prepare_input with.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ce50975-d53a-457b-9913-eb7fc516699b",
   "metadata": {},
   "source": [
    "## Deployment & Batch Prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a18a497f-5a9f-4327-9e98-1aaf989498ad",
   "metadata": {},
   "source": [
    "### 1- Setup & Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a98ba048-0813-42e5-a19d-ae264f3a72e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import joblib\n",
    "import pandas as pd\n",
    "import traceback\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd5490b7-0693-4448-b542-cccc9feb070f",
   "metadata": {},
   "source": [
    "### 2- Paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "35228fb2-a73a-49b8-a805-1ad79edcaaae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define project paths\n",
    "DATA_CLEANED = \"../data/cleaned/Telco-Customer-Churn.csv\"   # cleaned dataset\n",
    "MODEL_PATH = \"../models/xgb_model.pkl\"                      # trained model\n",
    "FEATURES_PATH = \"models/feature_columns.json\"               # saved feature columns\n",
    "OUTPUT_PATH = \"../data/analysis/batch_predictions.csv\"\n",
    "\n",
    "# Ensure folders exist\n",
    "os.makedirs(\"../data/analysis\", exist_ok=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0472922-9132-42f7-a582-0f094c95c345",
   "metadata": {},
   "source": [
    "### 3- Load Model & Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5e846cfe-5d39-4f85-bb1c-06c76f077f23",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Model & feature columns loaded\n",
      "Model expects: 13601 features\n"
     ]
    }
   ],
   "source": [
    "# Load trained model\n",
    "model = joblib.load(MODEL_PATH)\n",
    "\n",
    "# Load feature column order\n",
    "with open(FEATURES_PATH, \"r\") as f:\n",
    "    FEATURE_COLS = json.load(f)\n",
    "\n",
    "print(\"✅ Model & feature columns loaded\")\n",
    "print(\"Model expects:\", len(FEATURE_COLS), \"features\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aabea7f5-4968-44d0-bc80-70d4627a9489",
   "metadata": {},
   "source": [
    "### 4- Load Cleaned Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a643ce13-b884-44e3-8b51-0b41c55d0e5e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset shape: (7043, 20)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>gender</th>\n",
       "      <th>SeniorCitizen</th>\n",
       "      <th>Partner</th>\n",
       "      <th>Dependents</th>\n",
       "      <th>tenure</th>\n",
       "      <th>PhoneService</th>\n",
       "      <th>MultipleLines</th>\n",
       "      <th>InternetService</th>\n",
       "      <th>OnlineSecurity</th>\n",
       "      <th>OnlineBackup</th>\n",
       "      <th>DeviceProtection</th>\n",
       "      <th>TechSupport</th>\n",
       "      <th>StreamingTV</th>\n",
       "      <th>StreamingMovies</th>\n",
       "      <th>Contract</th>\n",
       "      <th>PaperlessBilling</th>\n",
       "      <th>PaymentMethod</th>\n",
       "      <th>MonthlyCharges</th>\n",
       "      <th>TotalCharges</th>\n",
       "      <th>Churn</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Female</td>\n",
       "      <td>0</td>\n",
       "      <td>Yes</td>\n",
       "      <td>No</td>\n",
       "      <td>1</td>\n",
       "      <td>No</td>\n",
       "      <td>No phone service</td>\n",
       "      <td>DSL</td>\n",
       "      <td>No</td>\n",
       "      <td>Yes</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>Month-to-month</td>\n",
       "      <td>Yes</td>\n",
       "      <td>Electronic check</td>\n",
       "      <td>29.85</td>\n",
       "      <td>29.85</td>\n",
       "      <td>No</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Male</td>\n",
       "      <td>0</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>34</td>\n",
       "      <td>Yes</td>\n",
       "      <td>No</td>\n",
       "      <td>DSL</td>\n",
       "      <td>Yes</td>\n",
       "      <td>No</td>\n",
       "      <td>Yes</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>One year</td>\n",
       "      <td>No</td>\n",
       "      <td>Mailed check</td>\n",
       "      <td>56.95</td>\n",
       "      <td>1889.5</td>\n",
       "      <td>No</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Male</td>\n",
       "      <td>0</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>2</td>\n",
       "      <td>Yes</td>\n",
       "      <td>No</td>\n",
       "      <td>DSL</td>\n",
       "      <td>Yes</td>\n",
       "      <td>Yes</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>Month-to-month</td>\n",
       "      <td>Yes</td>\n",
       "      <td>Mailed check</td>\n",
       "      <td>53.85</td>\n",
       "      <td>108.15</td>\n",
       "      <td>Yes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Male</td>\n",
       "      <td>0</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>45</td>\n",
       "      <td>No</td>\n",
       "      <td>No phone service</td>\n",
       "      <td>DSL</td>\n",
       "      <td>Yes</td>\n",
       "      <td>No</td>\n",
       "      <td>Yes</td>\n",
       "      <td>Yes</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>One year</td>\n",
       "      <td>No</td>\n",
       "      <td>Bank transfer (automatic)</td>\n",
       "      <td>42.30</td>\n",
       "      <td>1840.75</td>\n",
       "      <td>No</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Female</td>\n",
       "      <td>0</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>2</td>\n",
       "      <td>Yes</td>\n",
       "      <td>No</td>\n",
       "      <td>Fiber optic</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>Month-to-month</td>\n",
       "      <td>Yes</td>\n",
       "      <td>Electronic check</td>\n",
       "      <td>70.70</td>\n",
       "      <td>151.65</td>\n",
       "      <td>Yes</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   gender  SeniorCitizen Partner Dependents  tenure PhoneService  \\\n",
       "0  Female              0     Yes         No       1           No   \n",
       "1    Male              0      No         No      34          Yes   \n",
       "2    Male              0      No         No       2          Yes   \n",
       "3    Male              0      No         No      45           No   \n",
       "4  Female              0      No         No       2          Yes   \n",
       "\n",
       "      MultipleLines InternetService OnlineSecurity OnlineBackup  \\\n",
       "0  No phone service             DSL             No          Yes   \n",
       "1                No             DSL            Yes           No   \n",
       "2                No             DSL            Yes          Yes   \n",
       "3  No phone service             DSL            Yes           No   \n",
       "4                No     Fiber optic             No           No   \n",
       "\n",
       "  DeviceProtection TechSupport StreamingTV StreamingMovies        Contract  \\\n",
       "0               No          No          No              No  Month-to-month   \n",
       "1              Yes          No          No              No        One year   \n",
       "2               No          No          No              No  Month-to-month   \n",
       "3              Yes         Yes          No              No        One year   \n",
       "4               No          No          No              No  Month-to-month   \n",
       "\n",
       "  PaperlessBilling              PaymentMethod  MonthlyCharges TotalCharges  \\\n",
       "0              Yes           Electronic check           29.85        29.85   \n",
       "1               No               Mailed check           56.95       1889.5   \n",
       "2              Yes               Mailed check           53.85       108.15   \n",
       "3               No  Bank transfer (automatic)           42.30      1840.75   \n",
       "4              Yes           Electronic check           70.70       151.65   \n",
       "\n",
       "  Churn  \n",
       "0    No  \n",
       "1    No  \n",
       "2   Yes  \n",
       "3    No  \n",
       "4   Yes  "
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load cleaned dataset\n",
    "df_full = pd.read_csv(DATA_CLEANED)\n",
    "\n",
    "print(\"Dataset shape:\", df_full.shape)\n",
    "df_full.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7899d70-c8c9-4715-9626-4d2226a308cd",
   "metadata": {},
   "source": [
    "### 5- Prepare Input Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e279e709-685e-4ad7-b9a7-f634b833a8d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to align features exactly like training\n",
    "def prepare_input(df, feature_cols):\n",
    "    X = pd.get_dummies(df, drop_first=True)\n",
    "    X = X.reindex(columns=feature_cols, fill_value=0)  # align with training\n",
    "    return X\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22b3fc37-5752-4c03-b8ab-a2a2aa9985cd",
   "metadata": {},
   "source": [
    "### 6- Run Batch Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "3d74d20b-cdaa-44c9-bbcd-c8b1e49bf9eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved predictions for Tableau at: /Users/hrithik/customer-churn-Analysis/data/analysis/predictions_for_tableau.csv\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>customerid</th>\n",
       "      <th>predicted_churn</th>\n",
       "      <th>churn_probability</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0.32</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0.32</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0.42</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>0.43</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   customerid  predicted_churn  churn_probability\n",
       "0           1                0               0.25\n",
       "1           2                0               0.32\n",
       "2           3                0               0.32\n",
       "3           4                0               0.42\n",
       "4           5                0               0.43"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "import pickle\n",
    "\n",
    "# Load cleaned data\n",
    "df_full = pd.read_csv(DATA_CLEANED)\n",
    "\n",
    "# Detect ID column\n",
    "id_candidates = [c for c in df_full.columns if c.lower() in (\"customerid\",\"customer_id\",\"id\",\"custid\",\"cust_id\")]\n",
    "id_col = id_candidates[0] if id_candidates else \"customerid\"\n",
    "if not id_candidates:\n",
    "    df_full.insert(0, id_col, range(1, len(df_full)+1))\n",
    "\n",
    "# ---------------- Load feature columns ---------------- #\n",
    "# Try JSON first, fallback to Pickle\n",
    "try:\n",
    "    with open(MODEL_DIR / \"feature_cols.json\") as f:\n",
    "        feature_cols = json.load(f)\n",
    "except FileNotFoundError:\n",
    "    with open(MODEL_DIR / \"feature_cols.pkl\", \"rb\") as f:\n",
    "        feature_cols = pickle.load(f)\n",
    "\n",
    "# ---------------- Prepare input for prediction ---------------- #\n",
    "X_all = prepare_input(df_full.drop(columns=[\"churn\"], errors=\"ignore\"), feature_cols=feature_cols)\n",
    "\n",
    "# Ensure the DataFrame matches model features\n",
    "X_all = X_all.reindex(columns=feature_cols, fill_value=0)\n",
    "\n",
    "# ---------------- Predict churn ---------------- #\n",
    "probs = model.predict_proba(X_all)[:,1]\n",
    "preds = (probs >= 0.5).astype(int)\n",
    "\n",
    "# ---------------- Save predictions ---------------- #\n",
    "out = pd.DataFrame({\n",
    "    id_col: df_full[id_col],\n",
    "    \"predicted_churn\": preds,\n",
    "    \"churn_probability\": probs\n",
    "})\n",
    "\n",
    "out_path = ANALYSIS_DIR / \"predictions_for_tableau.csv\"\n",
    "out.to_csv(out_path, index=False)\n",
    "print(\"Saved predictions for Tableau at:\", out_path)\n",
    "\n",
    "# Display first rows\n",
    "out.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acd48eee-05a8-4b59-9d58-e4a378b93a8d",
   "metadata": {},
   "source": [
    "## 7) Write Flask API (api/app.py)\n",
    "This creates a small REST service with:\n",
    "- GET /ping (health)\n",
    "- POST /predict — accepts JSON object or list, returns predictions & probabilities\n",
    "We will write the file programmatically so you can deploy immediately.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "7d0d5c83-42ad-4247-9a01-305c6de4e590",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wrote /Users/hrithik/customer-churn-Analysis/api/app.py\n"
     ]
    }
   ],
   "source": [
    "\n",
    "flask_code = r\"\"\"\n",
    "from flask import Flask, request, jsonify\n",
    "import joblib, json, os, pandas as pd\n",
    "\n",
    "BASE_DIR = os.path.abspath(os.path.join(os.path.dirname(__file__), \"..\"))\n",
    "MODEL_PATHS = [\n",
    "    os.path.join(BASE_DIR, \"models\", \"churn_model.pkl\"),\n",
    "    os.path.join(BASE_DIR, \"models\", \"churn_model_rf.pkl\"),\n",
    "    os.path.join(BASE_DIR, \"models\", \"churn_model_xgb.pkl\")\n",
    "]\n",
    "# pick first existing\n",
    "model_path = next((p for p in MODEL_PATHS if os.path.exists(p)), None)\n",
    "if model_path is None:\n",
    "    raise FileNotFoundError(\"Model not found in models/\")\n",
    "\n",
    "model = joblib.load(model_path)\n",
    "with open(os.path.join(BASE_DIR, \"models\", \"feature_cols.pkl\"), \"r\") as f:\n",
    "    FEATURE_COLS = json.load(f)\n",
    "\n",
    "def prepare_input(df_input):\n",
    "    df = df_input.copy()\n",
    "    for c in df.select_dtypes(include=\"object\").columns:\n",
    "        df[c] = df[c].astype(str).str.strip()\n",
    "    X = pd.get_dummies(df, drop_first=True)\n",
    "    # reindex to feature cols\n",
    "    X = X.reindex(columns=FEATURE_COLS, fill_value=0)\n",
    "    return X\n",
    "\n",
    "app = Flask(__name__)\n",
    "\n",
    "@app.route(\"/ping\", methods=[\"GET\"])\n",
    "def ping():\n",
    "    return jsonify({\"status\":\"ok\"})\n",
    "\n",
    "@app.route(\"/predict\", methods=[\"POST\"])\n",
    "def predict():\n",
    "    try:\n",
    "        data = request.get_json()\n",
    "        if data is None:\n",
    "            return jsonify({\"error\":\"No JSON body provided\"}), 400\n",
    "        if isinstance(data, dict):\n",
    "            df = pd.DataFrame([data])\n",
    "        elif isinstance(data, list):\n",
    "            df = pd.DataFrame(data)\n",
    "        else:\n",
    "            return jsonify({\"error\":\"JSON must be object or list\"}), 400\n",
    "        X = prepare_input(df)\n",
    "        preds = model.predict(X).tolist()\n",
    "        probs = model.predict_proba(X)[:,1].tolist()\n",
    "        return jsonify({\"predictions\": preds, \"probabilities\": probs})\n",
    "    except Exception as e:\n",
    "        return jsonify({\"error\": str(e)}), 500\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    port = int(os.environ.get(\"PORT\", 5000))\n",
    "    app.run(host=\"0.0.0.0\", port=port, debug=True)\n",
    "\"\"\"\n",
    "(API_DIR / \"app.py\").write_text(flask_code, encoding=\"utf-8\")\n",
    "print(\"Wrote\", API_DIR / \"app.py\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0987c58b-1699-4894-afd2-0a7574ce49d7",
   "metadata": {},
   "source": [
    "## 8) Write Streamlit app (`streamlit_app.py`)\n",
    "The app:\n",
    "- will use the local model by default for offline demos\n",
    "- if `API_URL` env var is set, will call the remote Flask API instead\n",
    "- supports CSV upload (batch) and single-record sidebar inputs\n",
    "Write the file programmatically.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "e7106c03-f036-4dd6-92c3-ab7978470fdd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wrote streamlit_app.py\n"
     ]
    }
   ],
   "source": [
    "\n",
    "streamlit_code = r'''\n",
    "import os, joblib, json, pandas as pd, requests, streamlit as st\n",
    "\n",
    "st.set_page_config(page_title=\"Churn Predictor Demo\", layout=\"wide\")\n",
    "BASE_DIR = os.path.abspath(\".\")\n",
    "API_URL = os.environ.get(\"API_URL\")  # e.g., https://your-flask.onrender.com\n",
    "\n",
    "MODEL_PATHS = [\n",
    "    os.path.join(BASE_DIR, \"models\", \"churn_model.pkl\"),\n",
    "    os.path.join(BASE_DIR, \"models\", \"churn_model_rf.pkl\"),\n",
    "    os.path.join(BASE_DIR, \"models\", \"churn_model_xgb.pkl\")\n",
    "]\n",
    "def find_model():\n",
    "    for p in MODEL_PATHS:\n",
    "        if os.path.exists(p):\n",
    "            return p\n",
    "    return None\n",
    "\n",
    "model = None\n",
    "FEATURE_COLS = None\n",
    "if API_URL is None:\n",
    "    model_file = find_model()\n",
    "    if model_file is None:\n",
    "        st.error(\"No local model found in models/. Set API_URL to call remote API.\")\n",
    "    else:\n",
    "        model = joblib.load(model_file)\n",
    "        with open(os.path.join(BASE_DIR, \"models\", \"feature_cols.pkl\"), \"r\") as f:\n",
    "            FEATURE_COLS = json.load(f)\n",
    "\n",
    "def prepare_input_local(df):\n",
    "    df2 = df.copy()\n",
    "    for c in df2.select_dtypes(include=\"object\").columns:\n",
    "        df2[c] = df2[c].astype(str).str.strip()\n",
    "    X = pd.get_dummies(df2, drop_first=True)\n",
    "    X = X.reindex(columns=FEATURE_COLS, fill_value=0)\n",
    "    return X\n",
    "\n",
    "st.title(\"Churn Predictor — Demo\")\n",
    "st.markdown(\"Upload a CSV (same columns as training) or use the sidebar single-record form.\")\n",
    "\n",
    "uploaded = st.file_uploader(\"Upload CSV (for batch prediction)\", type=[\"csv\"])\n",
    "if uploaded is not None:\n",
    "    df = pd.read_csv(uploaded)\n",
    "    if API_URL:\n",
    "        payload = df.to_dict(orient=\"records\")\n",
    "        resp = requests.post(API_URL.rstrip(\"/\") + \"/predict\", json=payload, timeout=30)\n",
    "        if resp.status_code == 200:\n",
    "            res = resp.json()\n",
    "            df[\"predicted_churn\"] = res[\"predictions\"]\n",
    "            df[\"churn_probability\"] = res[\"probabilities\"]\n",
    "            st.success(\"Predictions fetched from API\")\n",
    "            st.dataframe(df.head(100))\n",
    "            st.download_button(\"Download predictions CSV\", df.to_csv(index=False).encode(), \"predictions.csv\")\n",
    "        else:\n",
    "            st.error(f\"API error: {resp.status_code}: {resp.text}\")\n",
    "    else:\n",
    "        X = prepare_input_local(df.drop(columns=[\"churn\"], errors=\"ignore\"))\n",
    "        df[\"predicted_churn\"] = model.predict(X)\n",
    "        df[\"churn_probability\"] = model.predict_proba(X)[:,1]\n",
    "        st.success(\"Local predictions ready\")\n",
    "        st.dataframe(df.head(100))\n",
    "        st.download_button(\"Download predictions CSV\", df.to_csv(index=False).encode(), \"predictions.csv\")\n",
    "\n",
    "st.sidebar.header(\"Single-record\")\n",
    "# Adjust fields based on your dataset: this is an example. Replace/extend with your columns.\n",
    "tenure = st.sidebar.number_input(\"tenure\", min_value=0, max_value=200, value=12)\n",
    "monthlycharges = st.sidebar.number_input(\"monthlycharges\", min_value=0.0, value=70.0)\n",
    "contract = st.sidebar.selectbox(\"contract\", [\"Month-to-month\", \"One year\", \"Two year\"])\n",
    "paymentmethod = st.sidebar.selectbox(\"paymentmethod\", [\"Electronic check\", \"Mailed check\", \"Bank transfer (automatic)\", \"Credit card (automatic)\"])\n",
    "\n",
    "if st.sidebar.button(\"Predict single\"):\n",
    "    payload = {\"tenure\": tenure, \"monthlycharges\": monthlycharges, \"contract\": contract, \"paymentmethod\": paymentmethod}\n",
    "    if API_URL:\n",
    "        resp = requests.post(API_URL.rstrip(\"/\") + \"/predict\", json=payload, timeout=15)\n",
    "        if resp.status_code == 200:\n",
    "            r = resp.json()\n",
    "            prob = r[\"probabilities\"][0]\n",
    "            st.metric(\"Churn probability\", f\"{prob:.2%}\")\n",
    "        else:\n",
    "            st.error(\"API error: \" + resp.text)\n",
    "    else:\n",
    "        df_single = pd.DataFrame([payload])\n",
    "        Xs = prepare_input_local(df_single)\n",
    "        prob = model.predict_proba(Xs)[:,1][0]\n",
    "        st.metric(\"Churn probability\", f\"{prob:.2%}\")\n",
    "'''\n",
    "Path(\"streamlit_app.py\").write_text(streamlit_code, encoding=\"utf-8\")\n",
    "print(\"Wrote streamlit_app.py\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac35ec04-2c0f-4a79-bffc-5234e9480199",
   "metadata": {},
   "source": [
    "## 9) Write `requirements.txt` `\n",
    "`requirements.txt` will be used by Render / Streamlit Cloud to install dependencies.\n",
    "`Procfile` is useful for Heroku or some other hosts.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "5a803be1-2624-4833-a860-28aa2285cb12",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wrote requirements.txt\n",
      "Wrote Procfile\n"
     ]
    }
   ],
   "source": [
    "# Cell 7 — requirements + procfile\n",
    "reqs = [\n",
    "    \"pandas\",\n",
    "    \"numpy\",\n",
    "    \"scikit-learn\",\n",
    "    \"joblib\",\n",
    "    \"flask\",\n",
    "    \"streamlit\",\n",
    "    \"gunicorn\",\n",
    "    \"requests\",\n",
    "    \"xgboost\"   # optional if your saved model needs it\n",
    "]\n",
    "(Path(\"requirements.txt\")).write_text(\"\\n\".join(reqs))\n",
    "print(\"Wrote requirements.txt\")\n",
    "\n",
    "proc_text = \"web: gunicorn api.app:app\\n\"\n",
    "(Path(\"Procfile\")).write_text(proc_text)\n",
    "print(\"Wrote Procfile\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "398f8ed2-a4e7-433a-b1c0-66c5636f2ca1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
